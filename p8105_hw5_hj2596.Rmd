---
title: "Homework 5"
author: "Hongji Jiang"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(1)
```


### Problem 0

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files. This was not prepared as a GitHub repo.

```{r load_libraries}
library(tidyverse)
```


## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("./data/"),
    path = str_c("./data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
full_df
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
tidy_df
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way.

##Problem 2

```{r}
homicide_data = read_csv("./data/homicide-data.csv")
```
This dataset has `r nrow(homicide_data)` rows and `r ncol(homicide_data)` columns. The columns(variables) are `r names(homicide_data)` and each row is a homicide incidnet.

```{r}
homicide_data=
  homicide_data %>%
  mutate(city_state = str_c(city, state, sep = ", "))
```
Created a new variable `city_state` by concatenating city and state, seperateing with ','.

```{r}
cities=
  homicide_data %>%
  mutate(
    unsolved = case_when(
      disposition == "Open/No arrest" ~ 1,
      disposition == "Closed without arrest" ~ 1,
      disposition == "Closed by arrest" ~ 0,
    )
  ) %>%
  group_by(city) %>%
  summarize(
    num_case = n(),
    num_unsolved_case = sum(unsolved)
  )
```
Summarize within cities to obtain the total number of homicides and the number of unsolved homicides 

```{r}
cities %>% 
  filter(city == "Baltimore")
```
Filter data only with the city Baltimore

```{r}
proportion_baltimore <-prop.test(
  cities%>%
    filter(city == "Baltimore") %>% 
    pull(num_unsolved_case), 
  cities%>%
    filter(city == "Baltimore") %>% 
    pull(num_case))%>%
  broom::tidy()
proportion_baltimore %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)
```
The code chunk above used `prop.test` function and `broom::tidy` to get the estimated proportion of homicides that are unsolved among all the hoicide cases in Baltimore, MD. 
The estimate proportion is 0.646. And the condience interval is (0.628, 0.663).

```{r}
all_cities = 
  cities %>%
  mutate(
    all_cities_proportion = map2(.x = num_unsolved_case, .y = num_case, ~prop.test(x = .x, n = .y)),
    all_cities_proportion = map(all_cities_proportion, broom::tidy)) %>% 
  unnest(cols = c(all_cities_proportion)) %>%
  select(city, num_unsolved_case, num_case, estimate, conf.low, conf.high)
all_cities %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)
```
The code chunk above used `prop.test` function, `map2`, `map` and `broom::tidy` to get the estimated proportion of unresolved homicides in all cities.

```{r}
all_cities%>%
  mutate(city = fct_reorder(city, estimate)) %>%
  ggplot(aes(x = city, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(y = "Proportion of Unsolved Homicide Cases", x = "City", title = "Proportion of Unsolved Homicide Cases by City") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)
  ) 
```
Create a plot that shows the estimated proportion of unresolved homocide and CIs for each city and add the errorbars based on the lower and upper limits.

##Problem 3

```{r}
test_simulation = function(mean_input) {
  vector = rnorm(30, mean = mean_input, sd = 5)
  
  t.test(vector) %>% 
    broom::tidy() %>% 
    select(estimate, pvalue = p.value)
}

```
Create a function that generate dataset from the normal distribution model given and obtain the estimate and p-value.

```{r}
result1 = 
  expand_grid(
    set_mean = 0,
    iteration = 1:5000
    
  ) %>% 
  mutate(
    testresult = map(set_mean, test_simulation)
  ) %>% 
  unnest(testresult)
```
Generate 5000 datasets from the model.

```{r}
result2 = 
  expand_grid(
    set_mean = 1:6,
    iteration = 1:5000
    
  ) %>% 
  mutate(
    testresult = map(set_mean, test_simulation)
  ) %>% 
  unnest(testresult)
```
Repeat the above for Î¼={1,2,3,4,5,6}

```{r}
data_for_plot = 
  result2 %>% 
  group_by(set_mean) %>% 
  mutate(
    null_rejected = ifelse(pvalue < 0.05, 1, 0),
    power = mean(null_rejected)
  ) 
data_for_plot %>% 
  ggplot(aes(x = set_mean, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = " Effect Size vs Power",
    x = "The true mean setted",
    y = "The corresponding power"
  )
```

```{r}
avg_est = data_for_plot %>% 
  group_by(set_mean) %>%
  mutate(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = set_mean, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  scale_x_continuous( breaks = 1:6 ) +
  scale_y_continuous( breaks = 1:6 ) +
  labs(
    x = "True Mean",
    y = "Average Estimate of Mean"
  ) 
avg_est

reject_null = data_for_plot %>% 
  filter(null_rejected == 1) %>%
  group_by(set_mean) %>%
  mutate(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = set_mean, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  scale_x_continuous( breaks = 1:6 ) +
  scale_y_continuous( breaks = 1:6 ) +
  labs(
    x = "Mean",
    y = "Average Estimate of Mean of Null Rejected"
  ) 

reject_null
```
In the first graph, we can see the average estimate of mu on the y axis and the true value of mu on the x axis are very close to each other.
In the second graph, we can observe that in the samples where the null is rejected, when the mu is equal to or smaller than 3, the sample average of mu is obviously larger than the true value of mu. When mu is equal to or larger than 4, it is not obviously different from the true value of mu.
We know that the power is relatively lower when mu is less than 3. And
this is why the sample average is higher than the true mu value when the mu we set is less than or equal to 3. When the power is lower, the estimate on the mu is less precise.
